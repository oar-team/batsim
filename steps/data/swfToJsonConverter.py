#!/usr/bin/python3

# Dependency : sortedcontainers
#    - installation: pip install sortedcontainers
# Everything else should be in the standard library
# Tested on cpython 3.4.3

from enum import Enum, unique
from sortedcontainers import SortedSet
import argparse
import re
import csv
import json
import sys
import datetime
import random

@unique
class SwfField(Enum):
	JOB_ID=1
	SUBMIT_TIME=2
	WAIT_TIME=3
	RUN_TIME=4
	ALLOCATED_PROCESSOR_COUNT=5
	AVERAGE_CPU_TIME_USED=6
	USED_MEMORY=7
	REQUESTED_NUMBER_OF_PROCESSORS=8
	REQUESTED_TIME=9
	REQUESTED_MEMORY=10
	STATUS=11
	USER_ID=12
	GROUP_ID=13
	APPLICATION_ID=14
	QUEUD_ID=15
	PARTITION_ID=16
	PRECEDING_JOB_ID=17
	THINK_TIME_FROM_PRECEDING_JOB=18

parser = argparse.ArgumentParser(description='Reads a SWF (Standard Workload Format) file and transform it into a JSON workload (used by Batsim)')
parser.add_argument('inputSWF', type=argparse.FileType('r'), help='The input SWF file')
parser.add_argument('outputJSON', type=str, help='The output JSON files (list of strings, split by ' '). They are generated by varying cpuFactor first then comFactor second (in the given order)')
parser.add_argument('-csv', '--outputCSV', type=argparse.FileType('w'), default=None, help='The output CSV file (only works if there is only one json file to generate)')
parser.add_argument('-i', '--indent', type=int, default=None, help='If set to a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0, or negative, will only insert newlines. The default value (None) selects the most compact representation.')
parser.add_argument('-cpu', '--cpuFactor', type=str, default='1e6', help='These numbers (list of floats, splits by ' ') are used to generate the amount of work for each resource')
parser.add_argument('-com', '--comFactor', type=str, default='1e6', help='These numbers (list of floats, splits by ' ') are used to generate the amount of communication between each pair of resources')
parser.add_argument('-jg', '--jobGrain', type=int, default=1, help='Selects the level of detail we want for jobs. This parameter is used to group jobs that have close running time per resource')
parser.add_argument('-jwf', '--jobWalltimeFactor', type=float, default=2, help='Jobs walltimes are computed by the formula max(givenWalltime, jobWalltimeFactor*givenRuntime)')
parser.add_argument('-mjh', '--maximumJobHeight', type=int, default=None, help='The maximum autorized job height: the jobs that require more processors than this setting are discarded')
parser.add_argument('-prj', '--pickRandomJobs', type=int, default=None, help="If set, only some random-picked jobs will be converted. Otherwise, all valid jobs are converted.")
parser.add_argument('-crs', '--customRandomSeed', type=int, default=None, help="Initializes the random seed with a specific seed instead of the current system time")
parser.add_argument('-gwo', '--givenWalltimeOnly', action="store_true", help='If set, only the given walltime in the trace will be')
parser.add_argument('-fst', '--forceSubmissionTime', type=float, default=None, help='If set, force the submission time of all jobs to be equal to the given value')
parser.add_argument('-pf', '--platformSize', type=int, default=None, help='If set, the number of machines to put in the output JSON files is set by this parameter instead of taking the maximum job size')
parser.add_argument('--jobMinWidth', type=float, default=1, help='The job minimum width')
parser.add_argument('--jobMaxWidth', type=float, default=float('inf'), help='The job maximum width')
parser.add_argument('--randomizeCommunications', type=float, default=1, help='If set to a real between 0 and 1, the communication factor will be picked randomly via random.uniform(param*comFactor, comFactor)')

group = parser.add_mutually_exclusive_group()
group.add_argument("-v", "--verbose", action="store_true")
group.add_argument("-q", "--quiet", action="store_true")

args = parser.parse_args()

cpuFactors = [float(x) for x in str.split(args.cpuFactor)]
comFactors = [float(x) for x in str.split(args.comFactor)]
outputJsonFiles = str.split(args.outputJSON)

#print(cpuFactors)
#print(comFactors)

if args.jobMinWidth <= 0:
	print('Invalid jobMinWidth argument: must be strictly greater than 0')
	exit(1)

if args.jobMaxWidth < args.jobMinWidth:
	print('Invalid job width range: jobMaxWidth ({}) should be greater than or equal to jobMinWidth ({})'.format(args.jobMaxWidth, args.jobMinWidth))
	exit(1)

if len(outputJsonFiles) < 1:
	print('Invalid input: at least one JSON file should be generated')
	exit(1)

if len(outputJsonFiles) != len(cpuFactors) * len(comFactors):
	print('Invalid input: the number of output files ({}) does not match the number of cpuFactors multiplied by the number of comFactors ({}*{}={})'.format(len(outputJsonFiles),
		len(cpuFactors), len(comFactors), len(cpuFactors) * len(comFactors)))
	exit(1)

if (args.randomizeCommunications < 0) or (args.randomizeCommunications > 1):
	print('Invalid input: the randomizeCommunications argument must be between 0 and 1')
	exit(1)

instancesToGenerate = []
instanceNumber = 0
for cpuFactor in cpuFactors:
	for comFactor in comFactors:
		instancesToGenerate.append((cpuFactor, comFactor, outputJsonFiles[instanceNumber]))
		instanceNumber = instanceNumber + 1


element = '([-+]?\d+(?:\.\d+)?)'
r = re.compile('\s*' + (element + '\s+') * 17 + element + '\s*')

currentID = 0
version=0

# Let a job be a tuple (jobID, resCount, runTime, submitTime, profile, walltime)

jobs = []
profiles = SortedSet()

# Let's loop over the lines of the input file
for line in args.inputSWF:
	res = r.match(line)

	if res:
		jobID = (int(float(res.group(SwfField.JOB_ID.value))))
		resCount = int(float(res.group(SwfField.ALLOCATED_PROCESSOR_COUNT.value)))
		runTime = float(res.group(SwfField.RUN_TIME.value))
		submitTime = max(0,float(res.group(SwfField.SUBMIT_TIME.value)))
		wallTime = max(args.jobWalltimeFactor*runTime, float(res.group(SwfField.REQUESTED_TIME.value)))

		if args.givenWalltimeOnly:
			wallTime = float(res.group(SwfField.REQUESTED_TIME.value))

		if args.forceSubmissionTime != None:
			if args.forceSubmissionTime < 0:
				print('Invalid input: forceSubmissionTime cannot be negative')
				exit(1)
			submitTime = args.forceSubmissionTime

		if (args.maximumJobHeight == None) or (resCount <= args.maximumJobHeight): 
			if (resCount > 0) and (runTime >= args.jobMinWidth) and (runTime <= args.jobMaxWidth):
				profile = int((((runTime/resCount) // args.jobGrain)+1) * args.jobGrain)
				profiles.add(profile)

				job = (currentID, resCount, runTime, submitTime, profile, wallTime)
				currentID = currentID + 1
				jobs.append(job)
			elif args.verbose:
				print('Job {} has been discarded'.format(jobID))
		elif args.verbose:
			print('Job {} has been discarded'.format(jobID))

# Let's only pick some random jobs if needed
if args.pickRandomJobs != None:
	if args.pickRandomJobs <= 0:
		print('The value of the pickRandomJobs argument must be strictly positive')
		exit(0)
	random.seed(args.customRandomSeed)
	jobs = random.sample(jobs, args.pickRandomJobs)

	# Let's remove unused profiles
	usedProfiles = [profile for (jobID, resCount, runTime, submitTime, profile, wallTime) in jobs]
	profiles = SortedSet()

	for profile in usedProfiles:
		profiles.add(profile)


# Export CSV
if args.outputCSV and len(outputJsonFiles) == 1:
	writer = csv.DictWriter(args.outputCSV, fieldnames=['jobID', 'resCount', 'runTime', 'runTimePerResource', 'profile', 'wallTime'])
	writer.writeheader()
	for (jobID, resCount, runTime, submitTime, profile, wallTime) in jobs:
		writer.writerow({'jobID':jobID, 'resCount':resCount, 'runTime':runTime, 'runTimePerResource':runTime/resCount, 'profile':profile, 'wallTime':wallTime})

# Export JSON
# Let's generate a list of dictionaries for the jobs
djobs = list()
for (jobID, resCount, runTime, submitTime, profile, wallTime) in jobs:
	djobs.append({'id':jobID, 'subtime':submitTime, 'walltime':wallTime, 'res':resCount, 'profile': str(profile)})

# Let's generate a dict of dictionaries for the profiles

for (cpuFactor, comFactor, outputJsonFilename) in instancesToGenerate:
	dprofs = {}
	for profile in profiles:
		usedComFactor = random.uniform(args.randomizeCommunications*comFactor, comFactor)
		dprofs[str(profile)] = {'type':'msg_par_hg', 'cpu':profile*cpuFactor, 'com':profile*usedComFactor}

	platform_size = max([resCount for (jobID, resCount, runTime, submitTime, profile, wallTime) in jobs])
	if args.platformSize != None:
		if args.platformSize < 1:
			print('Invalid input: platform size must be strictly positive')
			exit(1)
		platform_size = args.platformSize

	data = {
		'version':version,
		'command':' '.join(sys.argv[:]),
		'date': datetime.datetime.now().isoformat(' '),
		'description':'this workload had been automatically generated',
		'nb_res': platform_size,
		'jobs':djobs,
		'profiles':dprofs }

	try:
		outFile = open(outputJsonFilename, 'w')
		json.dump(data, outFile, indent=args.indent)

		if not args.quiet:
			print('{} jobs and {} profiles had been created'.format(len(jobs), len(profiles)))

	except IOError:
		print('Cannot write file', outputJsonFilename)
